{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding user and product interaction\n",
    "\n",
    "VP of product in the company ABC has asked you to review how users interact with their online travel website.\n",
    "They store their data in JSON files. Each row in these files lists all the different cities that have been searched for by a user within the same session (as well as some other info about the user). \n",
    "\n",
    "Business questions:\n",
    "1. There was a bug in the code and one country didn't get logged. Can you guess which country was that? How?\n",
    "\n",
    "2. For each city, find the most likely city to be also searched for within the same session.\n",
    "3. Travel sites are browsed by two kinds of users. Users who are actually planning a trip and users who just dream about a vacation. The first ones have obviously a much higher purchasing intent. Users planning a trip often search for cities close to each other, while users who search for cities far away from each other are often just dreaming about a vacation. Based on this idea, can you come up with an algorithm that clusters sessions into two groups: high intent and low intent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# install packages if needed\n",
    "list.of.packages <- c(\"dplyr\",\"jsonlite\",\"arules\")\n",
    "new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]\n",
    "if(length(new.packages)) install.packages(new.packages, repos = \"http://cran.us.r-project.org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(dplyr)\n",
    "library(jsonlite)\n",
    "library(arules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load json and transform to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load json file\n",
    "json_data <- fromJSON(\"data/challenge_3.json\", flatten=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract values from user field\n",
    "aux <- data.frame(json_data$user[[1]])\n",
    "for(i in c(2:nrow(json_data))){\n",
    "  aux <- rbind(aux, data.frame(json_data$user[[i]]))\n",
    "}\n",
    "\n",
    "# bind user columns with the initial dataset\n",
    "json_data <- cbind(json_data, aux)\n",
    "\n",
    "# remove previous user column\n",
    "json_data$user <- NULL\n",
    "\n",
    "# convert all to dataframe columns\n",
    "json_data <- json_data%>%\n",
    "            mutate(\n",
    "              session_id = as.character(session_id),\n",
    "              unix_timestamp = as.character(unix_timestamp),\n",
    "              cities = as.character(cities)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Target: Guessing the missing Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>country</th><th scope=col>n</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>    </td><td>2820</td></tr>\n",
       "\t<tr><td>DE  </td><td>3638</td></tr>\n",
       "\t<tr><td>ES  </td><td>1953</td></tr>\n",
       "\t<tr><td>FR  </td><td>2298</td></tr>\n",
       "\t<tr><td>IT  </td><td>1882</td></tr>\n",
       "\t<tr><td>UK  </td><td>3555</td></tr>\n",
       "\t<tr><td>US  </td><td>3876</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " country & n\\\\\n",
       "\\hline\n",
       "\t      & 2820\\\\\n",
       "\t DE   & 3638\\\\\n",
       "\t ES   & 1953\\\\\n",
       "\t FR   & 2298\\\\\n",
       "\t IT   & 1882\\\\\n",
       "\t UK   & 3555\\\\\n",
       "\t US   & 3876\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "country | n | \n",
       "|---|---|---|---|---|---|---|\n",
       "|      | 2820 | \n",
       "| DE   | 3638 | \n",
       "| ES   | 1953 | \n",
       "| FR   | 2298 | \n",
       "| IT   | 1882 | \n",
       "| UK   | 3555 | \n",
       "| US   | 3876 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  country n   \n",
       "1         2820\n",
       "2 DE      3638\n",
       "3 ES      1953\n",
       "4 FR      2298\n",
       "5 IT      1882\n",
       "6 UK      3555\n",
       "7 US      3876"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Guessing the missing country\n",
    "mysteriours_country <- json_data%>%filter(country==\"\")\n",
    "\n",
    "# write cities in csv file\n",
    "write.table(mysteriours_country[,\"cities\"], \"data/mysteriours_country.csv\", quote=FALSE, row.names=FALSE, col.names=FALSE, sep=\",\")\n",
    "\n",
    "# list of distinct countries\n",
    "json_data%>%count(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'Montreal QC, Chicago IL'</li>\n",
       "\t<li>'Calgary AB, New York NY'</li>\n",
       "\t<li>'New York NY'</li>\n",
       "\t<li>'Toronto ON, New York NY'</li>\n",
       "\t<li>'New York NY'</li>\n",
       "\t<li>'New York NY'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'Montreal QC, Chicago IL'\n",
       "\\item 'Calgary AB, New York NY'\n",
       "\\item 'New York NY'\n",
       "\\item 'Toronto ON, New York NY'\n",
       "\\item 'New York NY'\n",
       "\\item 'New York NY'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'Montreal QC, Chicago IL'\n",
       "2. 'Calgary AB, New York NY'\n",
       "3. 'New York NY'\n",
       "4. 'Toronto ON, New York NY'\n",
       "5. 'New York NY'\n",
       "6. 'New York NY'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"Montreal QC, Chicago IL\" \"Calgary AB, New York NY\"\n",
       "[3] \"New York NY\"             \"Toronto ON, New York NY\"\n",
       "[5] \"New York NY\"             \"New York NY\"            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(mysteriours_country$cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">It's probably Canada. There are many Canadian cities (such as \"Toronto ON\", \"Montreal QC\" and \"Vancouver BC\", with 457, 334 and 207 searches respectively) in userâ€™s sessions with missing country field. In addition to this, Canada is one of the countries with many searched cities that does not appear in the list of distinct countries in the dataset. For more details check the bottom of the [Notebook 3.2](Challenge 3.2 - Understanding user and product interaction.ipynb)  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the cities searched in sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# writing transactions in a csv file\n",
    "write.table(json_data[,\"cities\"], \"data/cities.csv\", quote=FALSE, row.names=FALSE, col.names=FALSE, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Target: For each city, find the most likely city to be also searched for within the same session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The goal is**: For each city, find the most likely city to be also searched for within the same session.\n",
    "\n",
    "**Solution**: The safest way to solve this is to calculate the fequency and ratio by each cities relation. \n",
    "\n",
    "<span style=\"color:red\">Please, for this solution check the link to [Notebook 3.2](Challenge 3.2 - Understanding user and product interaction.ipynb)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Association Rule Learning: APriori\n",
    "Another way to analyze this kind of data is to transform the researched cities in a session to transactions, and then use association rules learning algorithms like Apriori to discover the frequent item sets (or cities sets) and find cities association."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transforming sessions to transactions\n",
    "items <- strsplit(json_data$cities,\",\")\n",
    "transactions <- as(items, \"transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apriori\n",
      "\n",
      "Parameter specification:\n",
      " confidence minval smax arem  aval originalSupport maxtime support minlen\n",
      "       0.05    0.1    1 none FALSE            TRUE       5   0.001      2\n",
      " maxlen target   ext\n",
      "     10  rules FALSE\n",
      "\n",
      "Algorithmic control:\n",
      " filter tree heap memopt load sort verbose\n",
      "    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n",
      "\n",
      "Absolute minimum support count: 20 \n",
      "\n",
      "set item appearances ...[0 item(s)] done [0.00s].\n",
      "set transactions ...[107 item(s), 20022 transaction(s)] done [0.00s].\n",
      "sorting and recoding items ... [86 item(s)] done [0.00s].\n",
      "creating transaction tree ... done [0.00s].\n",
      "checking subsets of size 1 2 3 4 done [0.00s].\n",
      "writing ... [581 rule(s)] done [0.00s].\n",
      "creating S4 object  ... done [0.00s].\n"
     ]
    }
   ],
   "source": [
    "# running apriori\n",
    "rules <- apriori(transactions, parameter = list(sup = 0.001, conf = 0.05, target=\"rules\",minlen=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    lhs                       rhs               support     confidence\n",
      "[1] { Hialeah FL}          => {Jacksonville FL} 0.001198681 1.0000000 \n",
      "[2] {Jacksonville FL}      => { Hialeah FL}     0.001198681 0.0591133 \n",
      "[3] { Miami FL}            => {Jacksonville FL} 0.001198681 1.0000000 \n",
      "[4] {Jacksonville FL}      => { Miami FL}       0.001198681 0.0591133 \n",
      "[5] { Bakersfield CA}      => {Los Angeles CA}  0.001897912 0.9743590 \n",
      "[6] { Saint Petersburg FL} => {Jacksonville FL} 0.002047747 1.0000000 \n",
      "    lift      count\n",
      "[1] 49.315271 24   \n",
      "[2] 49.315271 24   \n",
      "[3] 49.315271 24   \n",
      "[4] 49.315271 24   \n",
      "[5]  9.828018 38   \n",
      "[6] 49.315271 41   \n"
     ]
    }
   ],
   "source": [
    "# checking the assiciation rules\n",
    "inspect(head(rules))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "With this quick analysis we can see the strong relationship between Hialeah and Jacksonville cities, as well as the number of times this relationship happened and its proportion in the dataset. The interesting thing about this type of analysis besides highlighting the most representative association rules is that we can see not only the relation of one city to another but the relation of it to several cities in a single rule. \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Target:  An algorithm that clusters sessions into two groups: high intent and low intent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **latitude** and **longitude** information, for each one of the cities researched by user, calculate the pairwise the distance between them and after that extract some aggregated measure from distances, such as the mean, the standard deviation, the minimum and the maximum. \n",
    "\n",
    "Using a clustering algorithm, such as **K-mean**, we could segregate and visualize different users and combine them to the best high intend and low intend group."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
